current situation with vpg
the paddle gets close the ball but not close enough so it gets negative reward. this causes is it to get stuck in a loopwhere it learns to get closer but then unlearns that when it continues to receive negative reward. 

maybe this is why value functions are needed

the value function will reward it based on the action that it takes
so it will get punished when going down if the ball is above it because the 

new value - previous value = negative value
this is also known as the advantage


training longer with bigger batches seems to be working with vpg (no value func)
before moving on to a2c let me write some plotting and logging code
i will log collected reward per game (first to eleven points)


what if somehow i use one ai for both agents? would that make training faster
i can try

